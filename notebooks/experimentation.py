# -*- coding: utf-8 -*-
"""Experimentation2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mGCA1AZCILhx8XeeSsyrSlcwRcfMJpG7

# **1 Data Cleaning and Preprocessing**
"""

import pandas as pd
import re
from dateutil import parser

from google.colab import drive
drive.mount('/content/drive')

try:
  df=pd.read_pickle("/content/drive/MyDrive/Earnings call prediction/motley-fool-data.pkl")
except Exception as e:
  print(f"Error : {e}")

df.columns

print(f"Data shape: {df.shape}")
df.head(5)

def parse_date(date_str):
  try:
    clean_str=re.sub(r"\sE[DS]?T","",str(date_str))
    return parser.parse(clean_str)
  except:
    return None
df["Timestamp"]=df["date"].apply(parse_date)
df.head(10)

df=df.dropna(subset=["Timestamp"])
df.shape

df["Date_only"]=df["Timestamp"].dt.normalize()
df.head(5)

def clean_transcript(text):
  if not isinstance(text,str):
    return ""
  text = re.sub(r'(Prepared Remarks:|Questions and Answers:)', '', text, flags=re.IGNORECASE)   #headers-Prepared remarks, Q&A
  text = re.sub(r'Operator\n.*?\n', '', text, flags=re.IGNORECASE)   #operator word
  text = re.sub(r'\s+', ' ', text).strip()  #extra whitespace
  return text
df["Clean_Text"]=df['transcript'].apply(clean_transcript)
df["word_count"]=df["Clean_Text"].apply(lambda x: len(str(x).split()))
df=df[df["word_count"]>100]      #keeping text word count >100, deleting short or cancelled calls
output_path="/content/drive/MyDrive/Earnings call prediction/cleaned_transcripts.pkl"
df.to_pickle(output_path)
print(df.shape)
df.head(5)

"""# **2 Stock Price Reaction**"""

!pip install yfinance

import yfinance as yf
from datetime import timedelta
import time
import pandas as pd

df=pd.read_pickle("/content/drive/MyDrive/Earnings call prediction/cleaned_transcripts.pkl")

earliest_date = df['Date_only'].min()
print(f"Earliest Transcript Date found: {earliest_date}")
start_date_dynamic=(earliest_date-pd.Timedelta(days=30)).strftime("%Y-%m-%d")
print(f"Fetching stock history from: {start_date_dynamic}")

def clean_ticker(ticker_str):
  t=str(ticker_str).upper().strip()
  if ":" in t:    #if ticker is NASDAQ:AAPL
    t=t.split(":")[1].strip()
  t=t.replace("$","")
  return t
df["ticker_clean"]=df['ticker'].apply(clean_ticker)
print(f"unique tickers: {df["ticker_clean"].unique().tolist()}")

# This uses 'regex=False' to search for the literal '$' character
print(f"Number of rows in 'ticker_clean' still containing '$': {df['ticker_clean'].str.contains('$', regex=False).sum()}")

price_data={}
def get_price_reaction(row):
  ticker=row["ticker_clean"]
  call_date=row["Date_only"]
  if not ticker or not isinstance(ticker, str): return None
  if ticker not in price_data:
    try:
      stock=yf.Ticker(ticker)
      hist=stock.history(start=start_date_dynamic,end=None)
      if hist.empty:
        price_data[ticker] = None
        return None
      hist.index=hist.index.tz_localize(None)
      price_data[ticker]=hist
    except Exception as e:
      print(f"Error on: {ticker} Error:{e}")
      price_data[ticker]=None
      return None
  hist=price_data[ticker]
  if hist is None or hist.empty:
    return None
  try:
    if call_date<hist.index.min() or call_date>hist.index.max():
      return None
    idx=hist.index.get_indexer([call_date],method="nearest")[0]
    if idx+1>=len(hist):
      return None
    day_0=hist.iloc[idx]
    day_1=hist.iloc[idx+1]
    price_change=(day_1["Close"]-day_0["Open"])/day_0["Open"]
    return price_change
  except Exception:
    return None

print(f"Fetching data")
df["Price_change"]=df.apply(get_price_reaction,axis=1)
df.head(5)

num_failed_fetches = df['Price_change'].isnull().sum()
print(f"Number of rows where Price_change could not be fetched: {num_failed_fetches}")

df_cleaned = df.dropna(subset=['Price_change']).copy()
print(f"New DataFrame shape after dropping rows: {df_cleaned.shape}")

df.head(5)

original_df_path="/content/drive/MyDrive/Earnings call prediction/original_stock_price_reaction.csv"
final_df_path="/content/drive/MyDrive/Earnings call prediction/cleaned_stock_price_reaction.csv"   #for model training
df.to_csv(original_df_path,index=False)
df_cleaned.to_csv(final_df_path,index=False)

"""# **3 Exploratory Data Analysis**"""

import pandas as pd
import numpy as np

df_analysis=pd.read_csv("/content/drive/MyDrive/Earnings call prediction/cleaned_stock_price_reaction.csv")
df_analysis.head(5)

print(df_analysis.shape)
df_analysis.columns

df_analysis['Price_change'].describe()

extreme_gain_count=df_analysis[df_analysis["Price_change"]>0.1].shape[0]
extreme_loss_count=df_analysis[df_analysis["Price_change"]<-0.1].shape[0]
print(f"No. of Earning calls resulting extreme gain : {extreme_gain_count}")
print(f"No. of Earning calls resulting extreme loss : {extreme_loss_count}")

import matplotlib.pyplot as plt
import seaborn as sns

CURRENT_THRESHOLD = 0.10
LOWER_THRESHOLD = 0.05

plt.figure(figsize=(12,6))
sns.histplot(df_analysis["Price_change"],bins=100,kde=True,edgecolor="none",color="skyblue",alpha=0.7)
plt.axvline(CURRENT_THRESHOLD,color="red",linestyle="--",linewidth=2,label=f"+{CURRENT_THRESHOLD*100:.0f}% Threshold (10%)")
plt.axvline(-CURRENT_THRESHOLD,color="red",linestyle="--",linewidth=2,label=f"-{CURRENT_THRESHOLD*100:.0f}% Threshold (10%)")

plt.axvline(LOWER_THRESHOLD,color="green",linestyle=":",linewidth=1,label=f"+{LOWER_THRESHOLD*100:.0f}% Test Threshold (5%)")
plt.axvline(-LOWER_THRESHOLD,color="green",linestyle=":",linewidth=1,label=f"-{LOWER_THRESHOLD*100:.0f}% Test Threshold (5%)")

plt.title("Distribution of Price change day0 open to day 1 close",fontsize=16)
plt.xlabel("Price_change (in decimal)",fontsize=12)
plt.ylabel("Frequency",fontsize=12)
plt.grid(axis="y",alpha=0.5)
plt.legend()
plt.show()

THRESHOLD=0.05
conditions=[(df_analysis["Price_change"]>=THRESHOLD), (df_analysis["Price_change"]<=-THRESHOLD)]
values=["Gain","Loss"]
df_analysis["Target_Class"]=np.select(condlist=conditions,choicelist=values,default="Neutral")
print("Target_Class column created")

class_distribution_counts=df_analysis['Target_Class'].value_counts()
class_distribution_percent=class_distribution_counts.apply(lambda x:f"{(x/class_distribution_counts.sum())*100:2f}%")
print(f"class_distribution_counts are {class_distribution_counts}")
print(f"Class distribution percentage are {class_distribution_percent}")

"""# **4 Feature Engineering**

**Temporal Features**
"""

df_analysis['Date_only']=pd.to_datetime(df_analysis["Date_only"])
df_analysis["DateOfWeek"]=df_analysis["Date_only"].dt.day_of_week
df_analysis["Month"]=df_analysis["Date_only"].dt.month
df_analysis["Quarter"]=df_analysis["Date_only"].dt.quarter

"""**bold text**"""

!pip install category_encoders
from category_encoders import TargetEncoder

df_analysis["Target_gain_flag"]=(df_analysis["Target_Class"]=="Gain").astype(int)
encoder=TargetEncoder(cols=["ticker_clean"])
df_analysis["Ticker_gain_prob"]=encoder.fit_transform(df_analysis["ticker_clean"],df_analysis["Target_gain_flag"])

df_analysis["Target_loss_flag"]=(df_analysis["Target_Class"]=="Loss").astype(int)
encoder=TargetEncoder(cols=["ticker_clean"])
df_analysis["Ticker_loss_prob"]=encoder.fit_transform(df_analysis["ticker_clean"],df_analysis["Target_loss_flag"])
df_analysis.head(5)

df_analysis.drop(columns=["Target_gain_flag","Target_loss_flag","ticker_clean"],inplace=True)
df_analysis.head(5)

path="/content/drive/MyDrive/Earnings call prediction/final_dataframe_after_FE.csv"
df_analysis.to_csv(path)

"""**Sentiment scores**"""

import pandas as pd
import numpy as np

df_analysis=pd.read_csv("/content/drive/MyDrive/Earnings call prediction/final_dataframe_after_FE.csv")

!pip install transformers torch tqdm
! pip install torch==2.9.0 'torch_xla[tpu]==2.9.0' -f https://storage.googleapis.com/libtpu-releases/index.html
import torch_xla.core.xla_model as xm
import torch
device = xm.xla_device()
print("TPU device connected:", device)
t1 = torch.randn(3, 3, device=device)
t2 = torch.randn(3, 3, device=device)
print("\nExample computation on TPU:", t1 + t2)

import torch
from transformers import pipeline
from tqdm.auto import tqdm
import pandas as pd

classifier = pipeline("text-classification", model="ProsusAI/finbert", return_all_scores=True, device=device)

# Preparing Data
texts = df_analysis['Clean_Text'].astype(str).tolist()
batch_size = 32
results = []

print(f"Starting inference on {len(texts)} rows...")
for i in tqdm(range(0, len(texts), batch_size)):
  batch = texts[i : i + batch_size]
  batch_results = classifier(batch, truncation=True, max_length=512)
  results.extend(batch_results)

# Parsing Results into Columns. FinBERT outputs a list of dictionaries. Mapping them to columns
pos_scores = []
neg_scores = []
neu_scores = []

for row_result in results:
  scores_dict = {item['label']: item['score'] for item in row_result}
  pos_scores.append(scores_dict.get('positive', 0))
  neg_scores.append(scores_dict.get('negative', 0))
  neu_scores.append(scores_dict.get('neutral', 0))

#Adding to DataFrame
df_analysis['FinBERT_Positive'] = pos_scores
df_analysis['FinBERT_Negative'] = neg_scores
df_analysis['FinBERT_Neutral'] = neu_scores

df_analysis.drop(columns=['Clean_Text'], inplace=True)

print("Success! FinBERT scores added.")

df_analysis.head(5)

df_analysis.columns

df_analysis.drop(columns=['Unnamed: 0',"Ticker_gain_prob","Ticker_loss_prob"],inplace=True)
df_analysis.head(5)

def clean_ticker(ticker_str):
  t=str(ticker_str).upper().strip()
  if ":" in t:    #if ticker is NASDAQ:AAPL
    t=t.split(":")[1].strip()
  t=t.replace("$","")
  return t
df_analysis["ticker_clean"]=df_analysis['ticker'].apply(clean_ticker)
print(f"unique tickers: {df_analysis["ticker_clean"].unique().tolist()}")

df_analysis.head(5)

df_analysis.columns

path="/content/drive/MyDrive/Earnings call prediction/Dataframe_after_finbert.csv"
df_analysis.to_csv(path,index=False)

from sklearn.model_selection import train_test_split, TimeSeriesSplit
df_analysis = df_analysis.sort_values(by='Date_only').reset_index(drop=True)
cols_to_drop = [
    "Target_Class",
    "Price_change",
    "Date_only",
    "date",
    "Timestamp",
    "transcript",
    "exchange",
    "q",
    "ticker",
    "word_count"
]

X = df_analysis.drop(columns=[c for c in cols_to_drop if c in df_analysis.columns])
y = df_analysis["Target_Class"]

test_size = 0.3
split_index = int(len(df_analysis) * (1 - test_size))

X_train = X.iloc[:split_index]
X_test = X.iloc[split_index:]
y_train = y.iloc[:split_index]
y_test = y.iloc[split_index:]

print(f"Training data size: {len(X_train)}")
print(f"Test data size: {len(X_test)}")
print("\nFinal Features in X_train:", X_train.columns.tolist())

import pandas as pd
train_df = pd.concat([X_train, y_train], axis=1)
test_df = pd.concat([X_test, y_test], axis=1)
train_path="/content/drive/MyDrive/Earnings call prediction/train_dataset.csv"
test_path="/content/drive/MyDrive/Earnings call prediction/test_dataset.csv"
train_df.to_csv(train_path, index=False)
test_df.to_csv(test_path, index=False)

print("Train and Test datasets saved successfully!")
print(f"Train shape: {train_df.shape}")
print(f"Test shape: {test_df.shape}")

!pip install category_encoders
from category_encoders import TargetEncoder

gain_encoder = TargetEncoder(cols=['ticker_clean'])
loss_encoder = TargetEncoder(cols=['ticker_clean'])

y_train_gain = (y_train == 'Gain').astype(int)
y_train_loss = (y_train == 'Loss').astype(int)

# FITTING on X_train, TRANSFORM X_train
X_train['Ticker_Gain_Prob'] = gain_encoder.fit_transform(X_train['ticker_clean'], y_train_gain)
X_train['Ticker_Loss_Prob'] = loss_encoder.fit_transform(X_train['ticker_clean'], y_train_loss)

# TRANSFORM X_test
X_test['Ticker_Gain_Prob'] = gain_encoder.transform(X_test['ticker_clean'])
X_test['Ticker_Loss_Prob'] = loss_encoder.transform(X_test['ticker_clean'])

X_train.drop(columns=['ticker_clean'], inplace=True)
X_test.drop(columns=['ticker_clean'], inplace=True)

print("Target Encoding done correctly! No data leakage.")
print("X_train columns:", X_train.columns.tolist())

"""# **5.Feature Scaling**"""

from sklearn.preprocessing import StandardScaler
import joblib

scaler = StandardScaler()

# Fitting on Train, Transforming Train
X_train_scaled = scaler.fit_transform(X_train)
# Transforming Test
X_test_scaled = scaler.transform(X_test)

X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)
print("Data Scaling Complete.")

# SAVING THE COMPONENTS for Streamlit ---
# Save Encoders
joblib.dump(gain_encoder, 'ticker_gain_encoder.joblib')
joblib.dump(loss_encoder, 'ticker_loss_encoder.joblib')
# Save Scaler (
joblib.dump(scaler, 'standard_scaler.joblib')

print("Encoders and Scaler saved to .joblib files.")

import pandas as pd

train_final_df=X_train_scaled.copy()
test_final_df=X_test_scaled.copy()

# Adding the Target column to them to save
train_final_df['Target_Class'] = y_train.reset_index(drop=True)
test_final_df['Target_Class'] = y_test.reset_index(drop=True)

train_final_df.to_csv("/content/drive/MyDrive/Earnings call prediction/train_scaled_ready.csv", index=False)
test_final_df.to_csv("/content/drive/MyDrive/Earnings call prediction/test_scaled_ready.csv", index=False)

print("Scaled data saved! You are safe to start Hyperparameter Tuning.")

"""# **6 Model Training**"""

!pip install xgboost
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import RandomizedSearchCV
from xgboost import XGBClassifier
import joblib

# Encoding Target Labels (Gain/Loss/Neutral -> 0/1/2)
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)

# Saving the Label Encoder for streamlit
joblib.dump(le, 'target_label_encoder.joblib')
print("Target Labels encoded and encoder saved.")

tscv = TimeSeriesSplit(n_splits=5)
# Hyperparameter Grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 4, 5, 6, 8],
    'min_child_weight': [1, 3, 5, 7],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 0.1, 0.2]
}

# Initializing the Model
xgb = XGBClassifier(
    objective='multi:softprob',
    num_class=3,
    random_state=42,
    eval_metric='mlogloss'
)

#Randomized Search
random_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_grid,
    n_iter=50,
    scoring='f1_weighted',
    cv=tscv,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

print("Starting Hyperparameter Tuning...")
random_search.fit(X_train_scaled, y_train_encoded)

print("\nTuning Complete!")
print(f"Best Parameters: {random_search.best_params_}")
print(f"Best Weighted F1-Score: {random_search.best_score_:.4f}")

# Saving the Best Model
best_model = random_search.best_estimator_
joblib.dump(best_model, 'final_xgb_model.joblib')
print("Best model saved as 'final_xgb_model.joblib'")

X_train_scaled.shape

"""# **7 Model Testing and Evaluation**"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

y_pred = best_model.predict(X_test_scaled)
# Converting numerical predictions back to labels (0/1/2 -> Gain/Loss/Neutral)
y_test_decoded = le.inverse_transform(y_test_encoded)
y_pred_decoded = le.inverse_transform(y_pred)

print("\n--- FINAL CLASSIFICATION REPORT ---")
print(classification_report(y_test_decoded, y_pred_decoded))

cm = confusion_matrix(y_test_decoded, y_pred_decoded, labels=['Gain', 'Loss', 'Neutral'])
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Gain', 'Loss', 'Neutral'],
            yticklabels=['Gain', 'Loss', 'Neutral'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""# **8 Feature Importance**"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
importances = best_model.feature_importances_

# Mapping scores to column names
feature_names = X_train.columns
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Visualize
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(10), palette='viridis')
plt.title('Top 10 Most Important Features for Predicting Stock Movement')
plt.xlabel('Importance Score')
plt.ylabel('Feature Name')
plt.show()

# Printing the raw data
print(feature_importance_df.head(10))

"""# **10 Threshold Tuning**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score

# Getting Probabilities instead of Class Labels
# This returns an array with 3 columns: [Prob_Gain, Prob_Loss, Prob_Neutral]
y_probs = best_model.predict_proba(X_test_scaled)

print(f"Class Order: {le.classes_}")
gain_index = np.where(le.classes_ == 'Gain')[0][0]

# Testing Thresholds from 0.3 to 0.9
thresholds = np.arange(0.3, 0.95, 0.05)
precisions = []
recalls = []

# Ground truth for Gain (Binary: 1 if Gain, 0 if Loss/Neutral)
y_test_gain_binary = (y_test_decoded == 'Gain').astype(int)

for t in thresholds:
    # If Prob_Gain > t, predict 1, else 0
    custom_preds = (y_probs[:, gain_index] >= t).astype(int)
    # Calculating scores
    if np.sum(custom_preds) > 0: # Avoid division by zero
        p = precision_score(y_test_gain_binary, custom_preds)
        r = recall_score(y_test_gain_binary, custom_preds)
    else:
        p = 1.0
        r = 0.0

    precisions.append(p)
    recalls.append(r)

# Plotting the Trade-off of Precision and Recall
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions, label='Precision (Win Rate)', marker='o', color='green')
plt.plot(thresholds, recalls, label='Recall (Opportunity Capture)', marker='x', color='blue')
plt.title('Impact of Confidence Threshold on Trading Performance')
plt.xlabel('Probability Threshold required to predict "Gain"')
plt.ylabel('Score')
plt.legend()
plt.grid(True)
plt.show()


for t, p, r in zip(thresholds, precisions, recalls):
    print(f"Threshold: {t:.2f} | Precision: {p:.2f} | Recall: {r:.2f}")

loss_index = np.where(le.classes_ == 'Loss')[0][0]

# Testing Thresholds from 0.3 to 0.9
thresholds = np.arange(0.3, 0.95, 0.05)
precisions = []
recalls = []

# Ground truth for Gain (Binary: 1 if Gain, 0 if Loss/Neutral)
y_test_loss_binary = (y_test_decoded == 'Loss').astype(int)

for t in thresholds:
    # If Prob_Gain > t, predict 1, else 0
    custom_preds = (y_probs[:, loss_index] >= t).astype(int)
    # Calculating scores
    if np.sum(custom_preds) > 0: # Avoid division by zero
        p = precision_score(y_test_loss_binary, custom_preds)
        r = recall_score(y_test_loss_binary, custom_preds)
    else:
        p = 1.0
        r = 0.0

    precisions.append(p)
    recalls.append(r)

# Plotting the Trade-off of Precision and Recall
plt.figure(figsize=(10, 6))
plt.plot(thresholds, precisions, label='Precision (Win Rate)', marker='o', color='green')
plt.plot(thresholds, recalls, label='Recall (Opportunity Capture)', marker='x', color='blue')
plt.title('Impact of Confidence Threshold on Trading Performance')
plt.xlabel('Probability Threshold required to predict "Loss"')
plt.ylabel('Score')
plt.legend()
plt.grid(True)
plt.show()


for t, p, r in zip(thresholds, precisions, recalls):
    print(f"Threshold: {t:.2f} | Precision: {p:.2f} | Recall: {r:.2f}")

"""**Download Files and get versions for Streamlit**"""

import os
import zipfile
from IPython.display import FileLink

files_to_download = [
    'final_xgb_model.joblib',
    'standard_scaler.joblib',
    'target_label_encoder.joblib',
    'ticker_gain_encoder.joblib',
    'ticker_loss_encoder.joblib',
    'cleaned_stock_data.csv'
]

zip_filename = 'project_artifacts.zip'

print(f"Zipping {len(files_to_download)} files into '{zip_filename}'...")
missing_files = []

with zipfile.ZipFile(zip_filename, 'w') as zipf:
    for file in files_to_download:
        if os.path.exists(file):
            zipf.write(file)
            print(f"  - Added: {file}")
        else:
            missing_files.append(file)
            print(f"Warning: '{file}' not found. Did you save it?")

if len(missing_files) == len(files_to_download):
    print("\nError: No files found to zip. Please re-run the saving steps.")
else:
    print(f"\nZip file created: {zip_filename}")
    try:
        from google.colab import files
        files.download(zip_filename)
        print("Downloading via Google Colab...")
    except ImportError:
        print("Kaggle Detected. Click the link below to download:")
        display(FileLink(zip_filename))

import xgboost
import sklearn
import joblib
print(f"XGBoost Version: {xgboost.__version__}")
print(f"Scikit-Learn Version: {sklearn.__version__}")
print(f"Joblib Version: {joblib.__version__}")